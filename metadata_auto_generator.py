import re
import json
from pathlib import Path
from typing import Dict, List, Any

# Google AI ì—†ì´ ì‘ë™í•˜ëŠ” ë²„ì „
try:
    import google.generativeai as genai
    from config import GEMINI_API_KEY
    USE_AI = True
except ImportError:
    USE_AI = False
    print("âš ï¸ Google AI ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œë§Œ ì‘ë™í•©ë‹ˆë‹¤.")

class MetadataAutoGenerator:
    """MD íŒŒì¼ì— ë©”íƒ€ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ê³  ì‚½ì…í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        if USE_AI:
            genai.configure(api_key=GEMINI_API_KEY)
            self.model = genai.GenerativeModel("gemini-1.5-flash")
        else:
            self.model = None
        
        # íŒŒì¼ë³„ í˜ì´ì§€ URL ë§¤í•‘ (ê°„ë‹¨í•˜ê²Œ!)
        self.page_urls = {
            "cleaned_add_infotalk.md": "https://kakaobusiness.gitbook.io/main/ad/bizmessage/notice-friend",
            "cleaned_black_list.md": "https://kakaobusiness.gitbook.io/main/ad/bizmessage/notice-friend/audit/black-list", 
            "cleaned_white_list.md": "https://kakaobusiness.gitbook.io/main/ad/bizmessage/notice-friend/audit/white-list",
            "cleaned_content-guide.md": "https://kakaobusiness.gitbook.io/main/ad/bizmessage/notice-friend/infotalk/content-guide",
            "cleaned_message_yuisahang.md": "https://kakaobusiness.gitbook.io/main/ad/moment/start/messagead/operations",
            "cleaned_info_simsa.md": "https://kakaobusiness.gitbook.io/main/ad/bizmessage/notice-friend/audit",
            "cleaned_alrimtalk.md": "https://kakaobusiness.gitbook.io/main/ad/bizmessage/notice-friend",
            "cleaned_message.md": "https://kakaobusiness.gitbook.io/main/channel/run/message",
            "cleaned_run_message.md": "https://kakaobusiness.gitbook.io/main/channel/run/message",
            "cleaned_zipguide.md": "https://kakaobusiness.gitbook.io/main/"
        }

        # íŒŒì¼ë³„ ê¸°ë³¸ ë©”íƒ€ë°ì´í„° í…œí”Œë¦¿ (ê¸°ì¡´ëŒ€ë¡œ)
        self.file_templates = {
            "cleaned_add_infotalk.md": {
                "file_type": "service_guide",
                "target_audience": "developer",
                "service": "ì•Œë¦¼í†¡",
                "compliance_required": True
            },
            "cleaned_black_list.md": {
                "file_type": "blacklist",
                "severity": "high", 
                "compliance_level": "mandatory"
            },
            "cleaned_white_list.md": {
                "file_type": "whitelist",
                "allowed": True,
                "compliance_level": "recommended"
            },
            "cleaned_content-guide.md": {
                "file_type": "content_guide",
                "template_specs": True
            },
            "cleaned_message_yuisahang.md": {
                "file_type": "compliance_guide",
                "legal_reference": "ì •ë³´í†µì‹ ë§ë²•"
            },
            "cleaned_info_simsa.md": {
                "file_type": "review_guide",
                "authority": "ì¹´ì¹´ì˜¤"
            }
        }
    
    def detect_content_patterns(self, content: str) -> Dict[str, Any]:
        """íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ìë™ ê°ì§€"""
        metadata = {}
        
        # ì„¹ì…˜ ë²ˆí˜¸ ê°ì§€
        section_match = re.search(r'## (\d+(?:\.\d+)*\.?\s*[^#\n]*)', content)
        if section_match:
            metadata["section"] = section_match.group(1).strip()
            
        # í•˜ìœ„ ì„¹ì…˜ ê°ì§€
        subsection_match = re.search(r'### (\d+(?:-\d+)+\.?\s*[^#\n]*)', content)
        if subsection_match:
            metadata["subsection"] = subsection_match.group(1).strip()
            
        # í˜ì´ì§€ ì •ë³´ ìë™ ê°ì§€ (ì„¹ì…˜ ê¸°ë°˜)
        page_info = self._detect_page_info(content)
        if page_info:
            metadata["page_reference"] = page_info
            
        # ì‹¬ê°ë„ ê°ì§€
        if any(word in content for word in ["ì˜êµ¬ì ìœ¼ë¡œ", "ì°¨ë‹¨", "ì¤‘ì§€", "ê¸ˆì§€"]):
            metadata["severity"] = "critical"
        elif any(word in content for word in ["ì œí•œ", "ë°˜ë ¤", "ìœ„ë°˜"]):
            metadata["severity"] = "high"
        elif any(word in content for word in ["ê¶Œì¥", "ì£¼ì˜", "í™•ì¸"]):
            metadata["severity"] = "medium"
        else:
            metadata["severity"] = "low"
            
        # ì½˜í…ì¸  íƒ€ì… ê°ì§€
        if "| " in content and "---|" in content:
            metadata["content_type"] = "table"
        elif re.search(r'{% hint style="danger" %}', content):
            metadata["content_type"] = "warning"
        elif re.search(r'{% hint style="success" %}', content):
            metadata["content_type"] = "example"
        elif re.search(r'{% hint style="info" %}', content):
            metadata["content_type"] = "info"
        elif any(word in content for word in ["ë‹¨ê³„", "ë°©ë²•", "ì ˆì°¨"]):
            metadata["content_type"] = "procedure"
        elif any(word in content for word in ["ì •ì˜", "ê°œë…", "ì´ë€"]):
            metadata["content_type"] = "definition"
        else:
            metadata["content_type"] = "general"
            
        # ì»´í”Œë¼ì´ì–¸ìŠ¤ ë ˆë²¨ ê°ì§€
        if any(word in content for word in ["í•„ìˆ˜", "ë°˜ë“œì‹œ", "ì˜ë¬´"]):
            metadata["compliance_level"] = "mandatory"
        elif any(word in content for word in ["ê¶Œì¥", "ë°”ëŒì§"]):
            metadata["compliance_level"] = "recommended"
        else:
            metadata["compliance_level"] = "optional"
            
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = []
        keyword_patterns = [
            r'ì•Œë¦¼í†¡', r'ì¹œêµ¬í†¡', r'ì •ë³´ì„±', r'ê´‘ê³ ì„±', r'í…œí”Œë¦¿', 
            r'ì‹¬ì‚¬', r'ìŠ¹ì¸', r'ë°˜ë ¤', r'ë°œì†¡', r'ì±„ë„',
            r'ì •ë³´í†µì‹ ë§ë²•', r'ì»´í”Œë¼ì´ì–¸ìŠ¤', r'ìœ„ë°˜', r'ì°¨ë‹¨'
        ]
        
        for pattern in keyword_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                keywords.append(pattern)
        
        metadata["keywords"] = keywords[:5]  # ìµœëŒ€ 5ê°œ
        
        return metadata
    
    def _detect_page_info(self, content: str) -> str:
        """ì½˜í…ì¸ ì—ì„œ í˜ì´ì§€ ì •ë³´ ìë™ ê°ì§€"""
        # íšŒì›ê°€ì… ê´€ë ¨
        if any(keyword in content for keyword in ["íšŒì›ê°€ì…", "ê°€ì…", "íšŒì›"]):
            return "íšŒì›ê°€ì… ê°€ì´ë“œ"
        # ì£¼ë¬¸/ë°°ì†¡ ê´€ë ¨  
        elif any(keyword in content for keyword in ["ì£¼ë¬¸", "ë°°ì†¡", "íƒë°°", "ê²°ì œ"]):
            return "ì£¼ë¬¸/ë°°ì†¡ ê°€ì´ë“œ"
        # ì˜ˆì•½ ê´€ë ¨
        elif any(keyword in content for keyword in ["ì˜ˆì•½", "ì‹ ì²­", "ë°©ë¬¸"]):
            return "ì˜ˆì•½/ì‹ ì²­ ê°€ì´ë“œ"
        # ì¿ í°/í¬ì¸íŠ¸ ê´€ë ¨
        elif any(keyword in content for keyword in ["ì¿ í°", "í¬ì¸íŠ¸", "ë§ˆì¼ë¦¬ì§€", "ì ë¦½"]):
            return "ì¿ í°/í¬ì¸íŠ¸ ê°€ì´ë“œ"
        # ê¸ˆìœµ ê´€ë ¨
        elif any(keyword in content for keyword in ["ì€í–‰", "ëŒ€ì¶œ", "ì´ì", "ì¦ê¶Œ", "ì¹´ë“œ"]):
            return "ê¸ˆìœµ ì„œë¹„ìŠ¤ ê°€ì´ë“œ"
        # ë³´ì•ˆ ê´€ë ¨
        elif any(keyword in content for keyword in ["ë³´ì•ˆ", "ì•ˆì „", "OTP", "ë¹„ë°€ë²ˆí˜¸"]):
            return "ë³´ì•ˆ/ì•ˆì „ ê°€ì´ë“œ"
        # ì‹¬ì‚¬ ê´€ë ¨
        elif any(keyword in content for keyword in ["ì‹¬ì‚¬", "ìŠ¹ì¸", "ë°˜ë ¤", "ê²€í† "]):
            return "ì‹¬ì‚¬ ì •ì±…"
        # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ê´€ë ¨
        elif any(keyword in content for keyword in ["ë¸”ë™ë¦¬ìŠ¤íŠ¸", "ê¸ˆì§€", "ë¶ˆê°€", "ìœ„ë°˜"]):
            return "ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì •ì±…"
        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ê´€ë ¨
        elif any(keyword in content for keyword in ["í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸", "í—ˆìš©", "ê°€ëŠ¥", "ë°œì†¡"]):
            return "í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì •ì±…"
        # í…œí”Œë¦¿ ì œì‘ ê´€ë ¨
        elif any(keyword in content for keyword in ["í…œí”Œë¦¿", "ì œì‘", "ê°€ì´ë“œ", "ìœ í˜•"]):
            return "í…œí”Œë¦¿ ì œì‘ ê°€ì´ë“œ"
        else:
            return "ì¼ë°˜ ê°€ì´ë“œ"
    
    def extract_metadata_with_ai(self, content_chunk: str) -> Dict[str, Any]:
        """AIë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        if not USE_AI or not self.model:
            # AI ì—†ì´ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "main_topic": "ì•Œë¦¼í†¡ ê°€ì´ë“œ",
                "purpose": "guide", 
                "business_impact": "medium",
                "tags": ["ì•Œë¦¼í†¡", "ê°€ì´ë“œ"]
            }
        
        try:
            prompt = f"""
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•´ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ ì¶”ì¶œí•´ì¤˜.
í•œêµ­ì–´ë¡œ ëœ ì•Œë¦¼í†¡ ê°€ì´ë“œ ë¬¸ì„œì˜ ì¼ë¶€ì•¼.

í…ìŠ¤íŠ¸:
{content_chunk[:1000]}...

ë‹¤ìŒ í˜•íƒœì˜ JSONìœ¼ë¡œ ì‘ë‹µí•´ì¤˜:
{{
    "main_topic": "ì£¼ìš” ì£¼ì œ (í•œêµ­ì–´)",
    "purpose": "ì´ ì„¹ì…˜ì˜ ëª©ì  (setup/guide/warning/example ì¤‘ í•˜ë‚˜)",
    "business_impact": "ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ë„ (high/medium/low)",
    "tags": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
}}
"""
            
            response = self.model.generate_content(prompt)
            result = json.loads(response.text.strip())
            return result
            
        except Exception as e:
            print(f"AI ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                "main_topic": "ë¯¸ë¶„ë¥˜",
                "purpose": "guide", 
                "business_impact": "medium",
                "tags": ["ì•Œë¦¼í†¡"]
            }
    
    def split_content_into_chunks(self, content: str) -> List[Dict[str, Any]]:
        """ì½˜í…ì¸ ë¥¼ ì˜ë¯¸ìˆëŠ” ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        
        # ì„¹ì…˜ë³„ë¡œ ë¶„í•  (## ë˜ëŠ” ### ê¸°ì¤€)
        sections = re.split(r'(^#{2,3}\s+.+$)', content, flags=re.MULTILINE)
        
        current_chunk = ""
        current_header = ""
        
        for i, section in enumerate(sections):
            if re.match(r'^#{2,3}\s+', section):  # í—¤ë”ì¸ ê²½ìš°
                if current_chunk.strip():  # ì´ì „ ì²­í¬ê°€ ìˆìœ¼ë©´ ì €ì¥
                    chunks.append({
                        "header": current_header,
                        "content": current_chunk.strip(),
                        "chunk_id": len(chunks) + 1
                    })
                
                current_header = section.strip()
                current_chunk = section + "\n"
            else:
                current_chunk += section
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk.strip():
            chunks.append({
                "header": current_header,
                "content": current_chunk.strip(),
                "chunk_id": len(chunks) + 1
            })
        
        return chunks
    
    def generate_metadata_for_chunk(self, chunk: Dict[str, Any], file_name: str) -> Dict[str, Any]:
        """ì²­í¬ë³„ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
        base_metadata = self.file_templates.get(file_name, {})
        
        # íŒ¨í„´ ê¸°ë°˜ ë©”íƒ€ë°ì´í„°
        pattern_metadata = self.detect_content_patterns(chunk["content"])
        
        # AI ê¸°ë°˜ ë©”íƒ€ë°ì´í„°
        ai_metadata = self.extract_metadata_with_ai(chunk["content"])
        
        # ì²­í¬ ê³ ìœ  ì •ë³´
        chunk_metadata = {
            "source_file": file_name,
            "chunk_id": chunk["chunk_id"],
            "header": chunk["header"],
            "content_length": len(chunk["content"])
        }
        
        # ëª¨ë“  ë©”íƒ€ë°ì´í„° ë³‘í•©
        final_metadata = {
            **base_metadata,
            **pattern_metadata, 
            **ai_metadata,
            **chunk_metadata
        }
        
        return final_metadata
    
    def insert_metadata_into_content(self, chunks: List[Dict[str, Any]], file_name: str) -> str:
        """ë©”íƒ€ë°ì´í„°ë¥¼ ì½˜í…ì¸ ì— ì‚½ì…"""
        result_content = []
        
        # ì²« ë²ˆì§¸ ì²­í¬ì—ë§Œ íŒŒì¼ ì „ì²´ ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for i, chunk in enumerate(chunks):
            metadata = self.generate_metadata_for_chunk(chunk, file_name)
            
            # ì²« ì²­í¬ì—ë§Œ source_url ì¶”ê°€
            if i == 0:
                metadata["source_url"] = self.page_urls.get(file_name, "")
            
            # ë©”íƒ€ë°ì´í„°ë¥¼ YAML í”„ë¡ íŠ¸ë§¤í„° í˜•ì‹ìœ¼ë¡œ ì‚½ì…
            metadata_block = "<!--\n"
            metadata_block += "METADATA:\n"
            for key, value in metadata.items():
                if isinstance(value, list):
                    metadata_block += f"  {key}: {json.dumps(value, ensure_ascii=False)}\n"
                else:
                    metadata_block += f"  {key}: {json.dumps(value, ensure_ascii=False)}\n"
            metadata_block += "-->\n\n"
            
            # ë©”íƒ€ë°ì´í„° + ì›ë³¸ ì½˜í…ì¸ 
            result_content.append(metadata_block + chunk["content"])
        
        return "\n\n".join(result_content)
    
    def process_file(self, file_path: str) -> None:
        """íŒŒì¼ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        path = Path(file_path)
        
        if not path.exists():
            print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return
        
        print(f"ğŸ”„ ì²˜ë¦¬ ì¤‘: {path.name}")
        
        # ì›ë³¸ íŒŒì¼ ì½ê¸°
        with open(path, 'r', encoding='utf-8') as f:
            original_content = f.read()
        
        # ì²­í¬ë¡œ ë¶„í• 
        chunks = self.split_content_into_chunks(original_content)
        print(f"ğŸ“ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        
        # ë©”íƒ€ë°ì´í„° ì‚½ì…
        enhanced_content = self.insert_metadata_into_content(chunks, path.name)
        
        # ë°±ì—… ìƒì„±
        backup_path = path.parent / f"{path.stem}_backup{path.suffix}"
        with open(backup_path, 'w', encoding='utf-8') as f:
            f.write(original_content)
        print(f"ğŸ’¾ ë°±ì—… ìƒì„±: {backup_path}")
        
        # ë©”íƒ€ë°ì´í„°ê°€ ì¶”ê°€ëœ íŒŒì¼ ì €ì¥
        with open(path, 'w', encoding='utf-8') as f:
            f.write(enhanced_content)
        
        print(f"âœ… ë©”íƒ€ë°ì´í„° ì‚½ì… ì™„ë£Œ: {path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - predata í´ë”ì˜ ëª¨ë“  MD íŒŒì¼ ì²˜ë¦¬"""
    generator = MetadataAutoGenerator()
    
    # predata í´ë”ì˜ ëª¨ë“  cleaned_*.md íŒŒì¼ ì°¾ê¸°
    predata_path = Path("predata")
    md_files = list(predata_path.glob("cleaned_*.md"))
    
    print(f"ğŸ“‚ {len(md_files)}ê°œì˜ MD íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    
    for md_file in md_files:
        # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì¸ì§€ í™•ì¸ (METADATA ì£¼ì„ì´ ìˆëŠ”ì§€ ì²´í¬)
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if "<!--\nMETADATA:" in content:
                print(f"â­ï¸  ì´ë¯¸ ì²˜ë¦¬ë¨: {md_file.name}")
                continue
                
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {md_file}: {e}")
            continue
            
        # íŒŒì¼ ì²˜ë¦¬
        generator.process_file(str(md_file))
    
    print(f"\nğŸ‰ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")

if __name__ == "__main__":
    main()